# Lesson17 大规模机器学习

## 17-1 大数据集学习
大量的数据产生的问题就是计算性能问题。

## 17-2 随机梯度下降

提出的目的是因为，当我们进行正常的梯度下降的时候，我们每次使用的是所有数据，如果数据量很大，那么就需要批次的将数据导入到内存，执行梯度下降，而这样的效率就很低。

而随机梯度下降算法，是逐个样本进行梯度下降，最后，直到部分的样本就能够完成这个过程，也不需要批量的导入。

所以算法如下:

	Repeat {
		for i=1 to m {
			θ_j := θj - α(h_θ(x^(i) ) - y^(i) )(x_j)^(i)
		}
	}
	
所以数据要随机化。

我觉得更好的随机化先进行聚类分析，然后再进行随机化，会使得数据更加随机.

实际上就是将梯度下降算法中的m=1来进行计算的。

![](../../res/quant/4_3.png)

## 17-3 最小批量梯度下降

最小批量梯度下降是使用一个比较小的样本来进行梯度下降。随机梯度下降中m=1,而标准的是m，那么小批量就是 b∈[2, 100] 使用这样一个小的样本数据集进行每次的梯度下降。

为什么小批量的梯度下降会更快一点的呢？是因为通过向量化，可以进行适当的并行计算，从而加速数据的计算。

## 17-4 随机梯度下降的收敛

这其实在前面已经阐述过，就是通过图像进行观察是否收敛。只是现在是通过每1000次迭代，来计算cost(θ, (x^\(i), y^\(i))的算术平均，来观察是否向下收敛。

另外关于学习速率α来说，是可以通过随着时间的变化来使得α变小来得到更加精准的拟合。

## 17-5 在线学习
在线学习系统，最大的有点在于不需要存储数据，随着每天的新的数据自动学习。这本质上就是随机梯度下降算法，每次来一个数据，就学习一次，来一个数据就学习一次。这不需要保存之前的数据，而是随着网站的运行自动的不断优化自身。这个算法很牛呀！！！

## 17-6 Map-reduce和数据并行

Map-reduce 能够处理非常大的数据。可以，通过分布式开源系统，例如Hadoop，来进行Map-reduce的实现。也可以，针对单一计算机的多核进行map reduce.其实,map reduce并非是关键的，关键的是 对算法的分解。



